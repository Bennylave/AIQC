{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "interesting-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-second",
   "metadata": {},
   "source": [
    "swap for the path to your repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sought-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/layne/Desktop/aiqc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "understanding-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "foreign-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiqc import datum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-scott",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-morris",
   "metadata": {},
   "source": [
    "making some dummy data for us to use as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "young-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datum.to_pandas('iris.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "genetic-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiqc.Dataset.Tabular.from_pandas(\n",
    "    dataframe = df\n",
    "    , name = 'tab separated plants'\n",
    "    , dtype = None #passed to pd.Dataframe(dtype)/ inferred\n",
    "    , column_names = None #passed to pd.Dataframe(columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "circular-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "preliminary-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset.make_label(columns=[label_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "frank-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = dataset.make_featureset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "greenhouse-reservoir",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = featureset.make_splitset(\n",
    "    label_id=label.id\n",
    "    , size_test=0.22 \n",
    "    , size_validation=0.12\n",
    "    , bin_count=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-aviation",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-particle",
   "metadata": {},
   "source": [
    "now that we have a splitset, let's take a look at the `samples` attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-surprise",
   "metadata": {},
   "source": [
    "because we specified a `size_validation` we have a validation entry in the samples dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "working-surveillance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['validation', 'train', 'test'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitset.samples.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-address",
   "metadata": {},
   "source": [
    "here are the sample indices that belong to the validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "extensive-chance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[140, 138, 106, 103, 13, 55, 68, 37, 53, 2, 69, 57, 117, 114, 42, 61, 28, 12]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitset.samples['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-sentence",
   "metadata": {},
   "source": [
    "when it comes time to read the splitset into memory, these indices are used to fetch the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-smell",
   "metadata": {},
   "source": [
    "the `to` methods allow you to filter what data you want. this comes in handy for downstream processes that need to select specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "handmade-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = splitset.to_numpy(\n",
    "    splits = ['train', 'test', 'validation']\n",
    "    , include_label = True\n",
    "    , include_featureset = True\n",
    "    , feature_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "certain-dressing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['validation', 'train', 'test'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "lined-length",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'labels'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data['validation'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-gazette",
   "metadata": {},
   "source": [
    "example of drilling down into the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "parallel-cooling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [2],\n",
       "       [2]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data['validation']['labels'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-burst",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-growth",
   "metadata": {},
   "source": [
    "things get more complicated when cross fold validation is introduced\n",
    "\n",
    "https://aiqc.readthedocs.io/en/latest/notebooks/api_low_level.html#5.-Optionally,-create-a-Foldset-for-cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "legal-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset = splitset.make_foldset(\n",
    "    fold_count=3\n",
    "    , bin_count=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-occurrence",
   "metadata": {},
   "source": [
    "The `Foldset` has no samples attribute. the sample indices live in the separate `Fold` objects, which are related to the `Foldset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "perceived-lincoln",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Fold: 4>, <Fold: 5>, <Fold: 6>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(foldset.folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "nuclear-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold4 = foldset.folds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "behavioral-amazon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold4.fold_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-theta",
   "metadata": {},
   "source": [
    "Each `Fold` has its own subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "entertaining-pulse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['folds_train_combined', 'fold_validation'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold4.samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "synthetic-niagara",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 4,\n",
       " 9,\n",
       " 13,\n",
       " 14,\n",
       " 16,\n",
       " 17,\n",
       " 19,\n",
       " 24,\n",
       " 26,\n",
       " 27,\n",
       " 32,\n",
       " 33,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 47,\n",
       " 49,\n",
       " 52,\n",
       " 53,\n",
       " 57,\n",
       " 60,\n",
       " 61,\n",
       " 63,\n",
       " 71,\n",
       " 72,\n",
       " 75,\n",
       " 76,\n",
       " 84,\n",
       " 85,\n",
       " 87,\n",
       " 95,\n",
       " 97]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold4.samples['fold_validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-hawaiian",
   "metadata": {},
   "source": [
    "But when it comes time to fetch the folds, they are called from the `Foldset` which also has some filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "macro-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "foldset_data = foldset.to_numpy(\n",
    "    fold_index = None\n",
    "    , fold_names = ['folds_train_combined', 'fold_validation']\n",
    "    , include_label = True\n",
    "    , include_featureset = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-insert",
   "metadata": {},
   "source": [
    "The `fold_index` is the first level of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "architectural-terrorism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldset_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "other-saying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['folds_train_combined', 'fold_validation'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldset_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "stable-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['features', 'labels'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldset_data[0]['folds_train_combined'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-failing",
   "metadata": {},
   "source": [
    "finally, the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "determined-shooting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldset_data[0]['folds_train_combined']['labels'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-lexington",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-telling",
   "metadata": {},
   "source": [
    "The question is, based on how the data in the `Batch` is designed, what splits/folds should be used for evaluating performance of the model during `job.run()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-photography",
   "metadata": {},
   "source": [
    "- Did the splitset use `size_validation`\n",
    "  - then don't evaluate on test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-miniature",
   "metadata": {},
   "source": [
    "- Did the `Batch` use a `foldset_id`\n",
    "  - then which `Fold` is related to this `Job`?\n",
    "  - then evaluate on that fold's 'fold_validation' instead of the test split or the validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-johns",
   "metadata": {},
   "source": [
    "A Batch can also be created with a `repeat_count` which determines how many duplicate runs there should be for each job."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
