{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "republican-adelaide",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-coordinate",
   "metadata": {},
   "source": [
    "Down the road, you will need to make real-life predictions using the models that you've trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-muslim",
   "metadata": {},
   "source": [
    "Inference is a breeze with AIQC because it persists all of the information that we need to preprocess our new samples and reconstruct our model.\n",
    "\n",
    "Normally, the challenge with inference is being able to preprocess your new samples the same way as your processed your training samples. Additionally, if you provide labels with your new data for the purpose of evaluation, then PyTorch requires you to reconstruct parts of your model like your optimizer in order to calculate loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-corpus",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "found-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiqc\n",
    "from aiqc import datum\n",
    "from aiqc import tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-alias",
   "metadata": {},
   "source": [
    "Below we're just making a trained model so that we have examples to work with for making inference-based predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "appreciated-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "queue_multiclass = tests.make_test_queue('keras_multiclass')\n",
    "queue_multiclass.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-stephen",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-appendix",
   "metadata": {},
   "source": [
    "Let's say that we have a trained model in the form of a `Predictor`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brilliant-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = queue_multiclass.jobs[0].predictors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-assessment",
   "metadata": {},
   "source": [
    "and that we have samples that we want to generate predictions for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-briefs",
   "metadata": {},
   "source": [
    "## New Splitset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "functional-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datum.to_pandas('iris.tsv').sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assured-buying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width     species\n",
       "63            6.1          2.9           4.7          1.4  versicolor\n",
       "2             4.7          3.2           1.3          0.2      setosa\n",
       "101           5.8          2.7           5.1          1.9   virginica\n",
       "83            6.0          2.7           5.1          1.6  versicolor\n",
       "74            6.4          2.9           4.3          1.3  versicolor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-discharge",
   "metadata": {},
   "source": [
    "We'll fashion a new `Splitset` of the samples that we want to predict using the high-level API.\n",
    "\n",
    "- Leave the `label_column` blank if you are conducting pure inference where you don't know the real Label/target.\n",
    "- Otherwise, `splitset.label` will be used to generate metrics for your new predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "embedded-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = aiqc.Pipeline.Tabular.make(\n",
    "    dataFrame_or_filePath = df\n",
    "    , label_column = 'species'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-leader",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-infrared",
   "metadata": {},
   "source": [
    "Then pass that `Splitset` to `Predictor.infer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-simple",
   "metadata": {},
   "source": [
    "During `infer`, it will validate that the schema of your new Splitset's `Feature` and `Label` match the schema of the original training Splitset. It will also ignore any splits that you make, fetching the entire Feature and Label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-relative",
   "metadata": {},
   "source": [
    "- `Dataset.Tabular` schema includes column ordering and dtype.\n",
    "- `Dataset.Image` schema includes Pillow size (height/width) and mode (color dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "quick-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.infer(splitset_id=splitset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-yeast",
   "metadata": {},
   "source": [
    "- The key in the dictionary-based `Prediction` attributes will be equal to the `str(splitset.id)`.\n",
    "- If you trained on encoded Labels, don't worry, the output will be `inverse_transform`'ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thermal-electronics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8': array(['versicolor', 'setosa', 'virginica', 'versicolor', 'versicolor',\n",
       "        'setosa', 'virginica', 'setosa', 'virginica', 'setosa'],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-burke",
   "metadata": {},
   "source": [
    "For more information on the `Prediction` object, reference the [Low-Level API](api_low_level.html) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
